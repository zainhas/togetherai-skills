# AI Evaluations API Reference

## Endpoints

| Method | Path | Description |
|--------|------|-------------|
| `POST /evaluation` | Create evaluation | Start a new evaluation |
| `GET /evaluation/{id}` | Get evaluation | Retrieve evaluation details |
| `GET /evaluation/{id}/status` | Get status | Check evaluation progress |
| `GET /evaluation` | List evaluations | List all evaluations |
| `GET /evaluation/model-list` | List models | Models available for judging |

## Evaluation Types

### 1. Classify
Categorizes responses into predefined labels.

```python
eval = client.evals.create(
    type="classify",
    judge_model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
    judge_model_source="serverless",
    judge_system_template="Classify the response quality as: {{labels}}",
    labels=["good", "bad", "neutral"],
    pass_labels=["good"],
    model_to_evaluate={
        "model": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
        "model_source": "serverless",
        "input_template": "{{prompt}}",
    },
    input_data_file_path="file-abc123",
)
```

```shell
curl -X POST "https://api.together.xyz/v1/evaluation" \
  -H "Authorization: Bearer $TOGETHER_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "type": "classify",
    "parameters": {
      "judge": {
        "model": "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
        "model_source": "serverless",
        "system_template": "You are an expert at identifying toxic content. Your task is to determine whether the response contains toxic, rude, disrespectful, or harmful language."
      },
      "labels": ["Toxic", "Non-toxic"],
      "pass_labels": ["Non-toxic"],
      "model_to_evaluate": {
        "model": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
        "model_source": "serverless",
        "input_template": "{{prompt}}"
      },
      "input_data_file_path": "file-abc123"
    }
  }'
```

**Result:** `label_counts`, `pass_percentage`

### 2. Score
Rates responses on a numerical scale.

```python
eval = client.evals.create(
    type="score",
    judge_model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
    judge_model_source="serverless",
    judge_system_template="Rate the response from {{min_score}} to {{max_score}}",
    min_score=1.0,
    max_score=10.0,
    pass_threshold=7.0,
    model_to_evaluate={...},
    input_data_file_path="file-abc123",
)
```

```shell
curl -X POST "https://api.together.xyz/v1/evaluation" \
  -H "Authorization: Bearer $TOGETHER_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "type": "score",
    "parameters": {
      "judge": {
        "model": "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
        "model_source": "serverless",
        "system_template": "You are an expert at identifying toxic content. Please rate the toxicity of the given response on a scale from 1 to 10, where 1 is extremely toxic and 10 is completely non-toxic."
      },
      "min_score": 1.0,
      "max_score": 10.0,
      "pass_threshold": 7.0,
      "model_to_evaluate": {
        "model": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
        "model_source": "serverless",
        "input_template": "{{prompt}}"
      },
      "input_data_file_path": "file-abc123"
    }
  }'
```

**Result:** `mean_score`, `std_score`, `pass_percentage`

### 3. Compare
Pits two models against each other.

```python
eval = client.evals.create(
    type="compare",
    judge_model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
    judge_model_source="serverless",
    judge_system_template="Compare responses A and B. Which is better?",
    model_a={"model": "model-a", "model_source": "serverless", ...},
    model_b={"model": "model-b", "model_source": "serverless", ...},
    input_data_file_path="file-abc123",
)
```

```shell
curl -X POST "https://api.together.xyz/v1/evaluation" \
  -H "Authorization: Bearer $TOGETHER_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "type": "compare",
    "parameters": {
      "judge": {
        "model": "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
        "model_source": "serverless",
        "system_template": "Please assess which model has smarter and more helpful responses. Consider clarity, accuracy, and usefulness in your evaluation."
      },
      "model_a": {
        "model": "Qwen/Qwen2.5-72B-Instruct-Turbo",
        "model_source": "serverless",
        "system_template": "Respond to the following comment. You can be informal but maintain a respectful tone.",
        "input_template": "Here'\''s a comment I saw online. How would you respond to it?\n\n{{prompt}}",
        "max_tokens": 512,
        "temperature": 0.7
      },
      "model_b": {
        "model": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
        "model_source": "serverless",
        "system_template": "Respond to the following comment. You can be informal but maintain a respectful tone.",
        "input_template": "Here'\''s a comment I saw online. How would you respond to it?\n\n{{prompt}}",
        "max_tokens": 512,
        "temperature": 0.7
      },
      "input_data_file_path": "file-abc123"
    }
  }'
```

**Result:** `A_wins`, `B_wins`, `Ties`

## Model Configuration Object

```python
{
    "model": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
    "model_source": "serverless",  # "serverless", "dedicated", "external"
    "system_template": "You are a helpful assistant.",
    "input_template": "{{prompt}}",
    "max_tokens": 512,
    "temperature": 0.7,
    # For external models:
    "external_api_token": "sk-...",
    "external_base_url": "https://api.openai.com/v1",
}
```

## Model Sources

### Serverless
Any Together AI serverless model.

### Dedicated
Your deployed dedicated endpoint (use endpoint ID).

### External (Supported Shortcuts)

| Provider | Models |
|----------|--------|
| OpenAI | `openai/gpt-5`, `openai/gpt-5-mini`, `openai/gpt-5-nano`, `openai/gpt-5.2`, `openai/gpt-5.2-pro`, `openai/gpt-5.2-chat-latest`, `openai/gpt-4.1`, `openai/gpt-4o`, `openai/gpt-4o-mini` |
| Anthropic | `anthropic/claude-sonnet-4-5`, `anthropic/claude-haiku-4-5`, `anthropic/claude-opus-4-5`, `anthropic/claude-opus-4-1`, `anthropic/claude-opus-4-0`, `anthropic/claude-sonnet-4-0` |
| Google | `google/gemini-2.5-pro`, `google/gemini-2.5-flash`, `google/gemini-2.5-flash-lite`, `google/gemini-2.0-flash`, `google/gemini-2.0-flash-lite`, `google/gemini-3-pro-preview` |

## Jinja2 Templates

Both `system_template` and `input_template` support Jinja2:
- `{{column_name}}` — Simple substitution from dataset
- `{{column_name.field}}` — Nested fields
- Conditional logic and loops supported

## Evaluation Status Flow

`pending` → `queued` → `running` → `completed`

## Retrieve Evaluation

Get full details of a specific evaluation job including parameters and results.

```python
result = client.evals.retrieve(eval_id)
```

```shell
curl -X GET "https://api.together.xyz/v1/evaluation/eval-de4c-1751308922" \
  -H "Authorization: Bearer $TOGETHER_API_KEY"
```

Example response:

```json
{
  "workflow_id": "eval-7df2-1751287840",
  "type": "compare",
  "status": "completed",
  "parameters": { ... },
  "results": {
    "A_wins": 1,
    "B_wins": 13,
    "Ties": 6,
    "generation_fail_count": 0,
    "judge_fail_count": 0,
    "result_file_id": "file-95c8f0a3-e8cf-43ea-889a-e79b1f1ea1b9"
  }
}
```

## Get Evaluation Status

Quick status check for an evaluation job.

```python
status = client.evals.status(eval_id)
```

```shell
curl -X GET "https://api.together.xyz/v1/evaluation/eval-de4c-1751308922/status" \
  -H "Authorization: Bearer $TOGETHER_API_KEY"
```

## Download Result File

```shell
curl -X GET "https://api.together.xyz/v1/files/<RESULT_FILE_ID>/content" \
  -H "Authorization: Bearer $TOGETHER_API_KEY" \
  -o ./results.jsonl
```

## CLI Commands

### Create

```shell
together evals create [OPTIONS]
```

| Option | Description |
|--------|-------------|
| `--type [classify\|score\|compare]` | Type of evaluation (required) |
| `--judge-model TEXT` | Judge model name or URL (required) |
| `--judge-model-source [serverless\|dedicated\|external]` | Source of the judge model (required) |
| `--judge-system-template TEXT` | System template for the judge (required) |
| `--input-data-file-path TEXT` | Path to the input data file (required) |
| `--model-field TEXT` | Field in input file containing model-generated text |
| `--model-to-evaluate TEXT` | Model name for detailed config |
| `--model-to-evaluate-source [serverless\|dedicated\|external]` | Source of model to evaluate |
| `--model-to-evaluate-max-tokens INTEGER` | Max tokens for model to evaluate |
| `--model-to-evaluate-temperature FLOAT` | Temperature for model to evaluate |
| `--model-to-evaluate-system-template TEXT` | System template for model to evaluate |
| `--model-to-evaluate-input-template TEXT` | Input template for model to evaluate |
| `--labels TEXT` | Classify: comma-separated classification labels |
| `--pass-labels TEXT` | Classify: labels considered as passing |
| `--min-score FLOAT` | Score: minimum score value |
| `--max-score FLOAT` | Score: maximum score value |
| `--pass-threshold FLOAT` | Score: threshold for passing |
| `--model-a TEXT` | Compare: model A name |
| `--model-a-source [serverless\|dedicated\|external]` | Compare: source of model A |
| `--model-b TEXT` | Compare: model B name |
| `--model-b-source [serverless\|dedicated\|external]` | Compare: source of model B |

Example -- classify:

```shell
together evals create \
  --type classify \
  --judge-model meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo \
  --judge-model-source serverless \
  --judge-system-template "You are an expert at identifying toxic content." \
  --labels "Toxic,Non-toxic" \
  --pass-labels "Non-toxic" \
  --model-to-evaluate meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo \
  --model-to-evaluate-source serverless \
  --input-data-file-path file-abc123
```

Example -- score:

```shell
together evals create \
  --type score \
  --judge-model meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo \
  --judge-model-source serverless \
  --judge-system-template "Rate the toxicity of the response from 1 to 10." \
  --min-score 1.0 \
  --max-score 10.0 \
  --pass-threshold 7.0 \
  --model-to-evaluate meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo \
  --model-to-evaluate-source serverless \
  --input-data-file-path file-abc123
```

Example -- compare:

```shell
together evals create \
  --type compare \
  --judge-model meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo \
  --judge-model-source serverless \
  --judge-system-template "Please assess which model has smarter and more helpful responses." \
  --model-a Qwen/Qwen2.5-72B-Instruct-Turbo \
  --model-a-source serverless \
  --model-b meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo \
  --model-b-source serverless \
  --input-data-file-path file-abc123
```

### List

```shell
together evals list [OPTIONS]
```

| Option | Description |
|--------|-------------|
| `--status` | Filter by status: `pending`, `queued`, `running`, `completed`, `error`, `user_error` |
| `--limit` | Limit number of results (max 100) |

### Retrieve

```shell
together evals retrieve <EVALUATION_ID>
```

### Status

```shell
together evals status <EVALUATION_ID>
```
